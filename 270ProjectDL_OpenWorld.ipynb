{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "270ProjectDL_OpenWorld.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYQlAGA2UHge"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as Data\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = pd.read_csv('./train_open.csv')\n",
        "testset = pd.read_csv('./test_open.csv')\n",
        "\n",
        "print(trainset)\n",
        "print(testset)\n",
        "\n",
        "X_train = trainset.iloc[:, 0:20].values\n",
        "Y_train = trainset.iloc[:, 20].values\n",
        "\n",
        "X_test = testset.iloc[:, 0:20].values\n",
        "Y_test = testset.iloc[:, 20].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
        "Y_train = torch.from_numpy(Y_train).type(torch.LongTensor)\n",
        "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
        "Y_test = torch.from_numpy(Y_test).type(torch.LongTensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrZaMIUDUfjV",
        "outputId": "7ec1a3e7-7f9f-49f6-efb7-6c06a956bf73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      cache-misses-1  node-loads-1  branch-misses-1  branch-load-misses-1  \\\n",
            "0           36218220       2530196         29315596              29316371   \n",
            "1           34025893       2310966         29030928              29031111   \n",
            "2           36778380       2597194         32242092              32242655   \n",
            "3           27574965       2081723         18248848              18249034   \n",
            "4           35736215       2499164         32035503              32038530   \n",
            "...              ...           ...              ...                   ...   \n",
            "1545        32765499       2249790         28748154              28751282   \n",
            "1546        36032964       2675011         23717334              23717871   \n",
            "1547        34176184       2447644         23153779              23154422   \n",
            "1548        32203221       2189984         28521743              28523470   \n",
            "1549        34253061       2361245         30368723              30370088   \n",
            "\n",
            "      cache-misses-2  node-loads-2  branch-misses-2  branch-load-misses-2  \\\n",
            "0           67010880       4826768         44219557              44218791   \n",
            "1           74840153       5778039         49779543              49780197   \n",
            "2           74405269       5584753         47564426              47564998   \n",
            "3           73767558       5803860         43940208              43940463   \n",
            "4           65822099       4772384         41975936              41972911   \n",
            "...              ...           ...              ...                   ...   \n",
            "1545        82592348       6463990         52916431              52915731   \n",
            "1546        77930191       6112430         47922023              47923542   \n",
            "1547        79152462       6068994         45784778              45784503   \n",
            "1548        79274149       6177813         52673555              52673172   \n",
            "1549        79908261       6256067         51440874              51441498   \n",
            "\n",
            "      cache-misses-3  node-loads-3  ...  branch-load-misses-3  cache-misses-4  \\\n",
            "0           61089052       4770075  ...              33063440        58901471   \n",
            "1           84125769       6507363  ...              41147892        47180922   \n",
            "2           64467520       4966192  ...              38899553        58727157   \n",
            "3           72955123       5991063  ...              32526988        59468415   \n",
            "4           64615266       4967699  ...              37184610        69232206   \n",
            "...              ...           ...  ...                   ...             ...   \n",
            "1545        69972572       5276739  ...              36389062        64962841   \n",
            "1546        73733629       6031949  ...              32944803        50344665   \n",
            "1547        77430746       6307641  ...              38594648        46941364   \n",
            "1548        75339650       5781138  ...              40070212        57134701   \n",
            "1549        69103527       5103737  ...              41377859        49707898   \n",
            "\n",
            "      node-loads-4  branch-misses-4  branch-load-misses-4  cache-misses-5  \\\n",
            "0          4235577         23518737              23517769        42228033   \n",
            "1          3188601         22359608              22358975        53289344   \n",
            "2          4140510         26356572              26357154        47354346   \n",
            "3          4568843         32546299              32547098        68949748   \n",
            "4          4963483         30828355              30828973        51816377   \n",
            "...            ...              ...                   ...             ...   \n",
            "1545       4761708         33874589              33867266        47447789   \n",
            "1546       3540111         25981418              25980070        46326190   \n",
            "1547       3405554         24044938              24043318        46725654   \n",
            "1548       4050827         32791802              32790501        48657736   \n",
            "1549       3414122         21085976              21085055        48424225   \n",
            "\n",
            "      node-loads-5  branch-misses-5  branch-load-misses-5  label  \n",
            "0          2981700         17816182              17816426      7  \n",
            "1          3827425         38928056              38927513     23  \n",
            "2          3340845         18700359              18699857     17  \n",
            "3          4417253         58589561              58588155      0  \n",
            "4          3675632         19138978              19137869     12  \n",
            "...            ...              ...                   ...    ...  \n",
            "1545       3374897         16673913              16673701     26  \n",
            "1546       3217768         16989584              16989614      1  \n",
            "1547       3306579         17000690              17000473      1  \n",
            "1548       3434079         17285141              17285080     26  \n",
            "1549       3449959         17671119              17671581     13  \n",
            "\n",
            "[1550 rows x 21 columns]\n",
            "     cache-misses-1  node-loads-1  branch-misses-1  branch-load-misses-1  \\\n",
            "0          33534639       2368872         22796799              22797179   \n",
            "1          32232524       2183296         28148918              28150145   \n",
            "2          26970152       1962052         17273316              17273348   \n",
            "3          33022157       2301241         27949457              27950095   \n",
            "4          30530041       2055189         25497203              25497316   \n",
            "..              ...           ...              ...                   ...   \n",
            "615        38171975       2742669         32189665              32191087   \n",
            "616        32279778       2202587         26713082              26714179   \n",
            "617        25598187       1914919         16633090              16633360   \n",
            "618        35783099       2524731         30651568              30652079   \n",
            "619        33492002       2357476         29188532              29189261   \n",
            "\n",
            "     cache-misses-2  node-loads-2  branch-misses-2  branch-load-misses-2  \\\n",
            "0          70491405       5571363         45438807              45439991   \n",
            "1          76925940       5729503         49921203              49920450   \n",
            "2          66149906       5036740         41494999              41495228   \n",
            "3          67314209       4813251         44448154              44447519   \n",
            "4          73335409       5329356         50222727              50223219   \n",
            "..              ...           ...              ...                   ...   \n",
            "615        70420773       5124793         46039463              46040717   \n",
            "616        81179111       6096084         52939317              52939088   \n",
            "617        66324639       5136472         42346187              42347119   \n",
            "618        64756446       4622112         40832170              40831659   \n",
            "619        68250729       4971700         44577279              44576650   \n",
            "\n",
            "     cache-misses-3  node-loads-3  ...  branch-load-misses-3  cache-misses-4  \\\n",
            "0          63848650       4933524  ...              32622917        78797245   \n",
            "1          69055005       5292372  ...              39405615        59060107   \n",
            "2          72247835       5830495  ...              36866418        71769005   \n",
            "3          42632597       2983742  ...              20771243        52042219   \n",
            "4          78397977       6121909  ...              43882288        74406504   \n",
            "..              ...           ...  ...                   ...             ...   \n",
            "615        76377171       5917658  ...              43122648        72545211   \n",
            "616        62124743       4673479  ...              40138287        58390361   \n",
            "617        79027933       6564622  ...              36322218        66702187   \n",
            "618        51721879       3779745  ...              27685264        59953751   \n",
            "619        64075211       4991918  ...              38925833        59626039   \n",
            "\n",
            "     node-loads-4  branch-misses-4  branch-load-misses-4  cache-misses-5  \\\n",
            "0         5766023         35649588              35651276        72198012   \n",
            "1         4121234         31725687              31724672        56674876   \n",
            "2         6094217         28593327              28595525        97611968   \n",
            "3         3838474         22377095              22377033        33993195   \n",
            "4         5196670         37276931              37276481        47815700   \n",
            "..            ...              ...                   ...             ...   \n",
            "615       5173016         33562165              33559599        47452445   \n",
            "616       4150812         27847639              27850221        44855595   \n",
            "617       5313780         31228163              31228906        80462464   \n",
            "618       4441445         28156119              28155955        41649164   \n",
            "619       4203048         26005460              26003860        50433679   \n",
            "\n",
            "     node-loads-5  branch-misses-5  branch-load-misses-5  label  \n",
            "0         5515362         22353340              22351242      3  \n",
            "1         3974914         24357365              24357368      9  \n",
            "2         7736877         42193461              42191184      0  \n",
            "3         2646994         16859897              16859615     20  \n",
            "4         3309081         17478460              17478380     30  \n",
            "..            ...              ...                   ...    ...  \n",
            "615       3276112         17276394              17275995     30  \n",
            "616       3139808         17592958              17590848     17  \n",
            "617       6363729         26995259              26994285      0  \n",
            "618       3224799         19006770              19007292      7  \n",
            "619       3491833         34301570              34302105     19  \n",
            "\n",
            "[620 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_set = Data.TensorDataset(X_train, Y_train)\n",
        "train_loader = Data.DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "FHiLfQen4Y2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(20, 48)\n",
        "        self.fc2 = nn.Linear(48, 96)\n",
        "        self.fc3 = nn.Linear(96, 128)\n",
        "        self.fc4 = nn.Linear(128, 31)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(self.fc4(x))\n",
        "        \n",
        "        return x\n",
        "    \n",
        "model = MLP()"
      ],
      "metadata": {
        "id": "V8n3ZZb2Wuaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5000\n",
        "learning_rate = 1e-3\n",
        "batch_no = len(X_train) // batch_size\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "T77hmkveWttv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    loss_sum = 0\n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        y_pred = model(x)\n",
        "        y = y.squeeze()\n",
        "        loss = loss_function(y_pred, y)\n",
        "        loss_sum += loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 50 == 0:\n",
        "        print(\"epoch: %d, loss: %f\" % (epoch, loss_sum/batch_size))\n",
        "        acc_sum = 0\n",
        "        acc_sum += (model(X_train).argmax(dim=1) == Y_train.squeeze()).sum()\n",
        "        print(\"train accuracy: %f\" % (acc_sum/len(Y_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "futCQSebY5_5",
        "outputId": "36d6c3e0-d791-4cf1-fcab-6aed6ee49047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 50, loss: 0.026867\n",
            "train accuracy: 0.740000\n",
            "epoch: 100, loss: 0.018779\n",
            "train accuracy: 0.779355\n",
            "epoch: 150, loss: 0.016925\n",
            "train accuracy: 0.802581\n",
            "epoch: 200, loss: 0.013197\n",
            "train accuracy: 0.850968\n",
            "epoch: 250, loss: 0.010303\n",
            "train accuracy: 0.864516\n",
            "epoch: 300, loss: 0.012773\n",
            "train accuracy: 0.859355\n",
            "epoch: 350, loss: 0.010843\n",
            "train accuracy: 0.881290\n",
            "epoch: 400, loss: 0.008273\n",
            "train accuracy: 0.881935\n",
            "epoch: 450, loss: 0.007460\n",
            "train accuracy: 0.903226\n",
            "epoch: 500, loss: 0.006823\n",
            "train accuracy: 0.906452\n",
            "epoch: 550, loss: 0.008396\n",
            "train accuracy: 0.909032\n",
            "epoch: 600, loss: 0.006360\n",
            "train accuracy: 0.910323\n",
            "epoch: 650, loss: 0.005765\n",
            "train accuracy: 0.920000\n",
            "epoch: 700, loss: 0.004978\n",
            "train accuracy: 0.922581\n",
            "epoch: 750, loss: 0.004699\n",
            "train accuracy: 0.936129\n",
            "epoch: 800, loss: 0.004893\n",
            "train accuracy: 0.925161\n",
            "epoch: 850, loss: 0.005239\n",
            "train accuracy: 0.920645\n",
            "epoch: 900, loss: 0.003759\n",
            "train accuracy: 0.932903\n",
            "epoch: 950, loss: 0.004596\n",
            "train accuracy: 0.919355\n",
            "epoch: 1000, loss: 0.004125\n",
            "train accuracy: 0.922581\n",
            "epoch: 1050, loss: 0.004107\n",
            "train accuracy: 0.922581\n",
            "epoch: 1100, loss: 0.004323\n",
            "train accuracy: 0.927742\n",
            "epoch: 1150, loss: 0.004831\n",
            "train accuracy: 0.930323\n",
            "epoch: 1200, loss: 0.004644\n",
            "train accuracy: 0.935484\n",
            "epoch: 1250, loss: 0.004136\n",
            "train accuracy: 0.929032\n",
            "epoch: 1300, loss: 0.004204\n",
            "train accuracy: 0.938065\n",
            "epoch: 1350, loss: 0.004165\n",
            "train accuracy: 0.934839\n",
            "epoch: 1400, loss: 0.004130\n",
            "train accuracy: 0.925161\n",
            "epoch: 1450, loss: 0.003502\n",
            "train accuracy: 0.933548\n",
            "epoch: 1500, loss: 0.006725\n",
            "train accuracy: 0.905161\n",
            "epoch: 1550, loss: 0.003022\n",
            "train accuracy: 0.920000\n",
            "epoch: 1600, loss: 0.003590\n",
            "train accuracy: 0.929677\n",
            "epoch: 1650, loss: 0.004215\n",
            "train accuracy: 0.923226\n",
            "epoch: 1700, loss: 0.003435\n",
            "train accuracy: 0.930968\n",
            "epoch: 1750, loss: 0.004062\n",
            "train accuracy: 0.927742\n",
            "epoch: 1800, loss: 0.002866\n",
            "train accuracy: 0.931613\n",
            "epoch: 1850, loss: 0.003368\n",
            "train accuracy: 0.933548\n",
            "epoch: 1900, loss: 0.003978\n",
            "train accuracy: 0.927742\n",
            "epoch: 1950, loss: 0.004635\n",
            "train accuracy: 0.928387\n",
            "epoch: 2000, loss: 0.002980\n",
            "train accuracy: 0.921935\n",
            "epoch: 2050, loss: 0.003559\n",
            "train accuracy: 0.925806\n",
            "epoch: 2100, loss: 0.003317\n",
            "train accuracy: 0.929677\n",
            "epoch: 2150, loss: 0.002973\n",
            "train accuracy: 0.938065\n",
            "epoch: 2200, loss: 0.003405\n",
            "train accuracy: 0.937419\n",
            "epoch: 2250, loss: 0.003588\n",
            "train accuracy: 0.935484\n",
            "epoch: 2300, loss: 0.003088\n",
            "train accuracy: 0.923226\n",
            "epoch: 2350, loss: 0.003806\n",
            "train accuracy: 0.935484\n",
            "epoch: 2400, loss: 0.003374\n",
            "train accuracy: 0.942581\n",
            "epoch: 2450, loss: 0.003205\n",
            "train accuracy: 0.926452\n",
            "epoch: 2500, loss: 0.003297\n",
            "train accuracy: 0.932903\n",
            "epoch: 2550, loss: 0.003211\n",
            "train accuracy: 0.927742\n",
            "epoch: 2600, loss: 0.003121\n",
            "train accuracy: 0.930323\n",
            "epoch: 2650, loss: 0.003746\n",
            "train accuracy: 0.930323\n",
            "epoch: 2700, loss: 0.004108\n",
            "train accuracy: 0.920000\n",
            "epoch: 2750, loss: 0.003859\n",
            "train accuracy: 0.922581\n",
            "epoch: 2800, loss: 0.003238\n",
            "train accuracy: 0.932903\n",
            "epoch: 2850, loss: 0.003128\n",
            "train accuracy: 0.947097\n",
            "epoch: 2900, loss: 0.003840\n",
            "train accuracy: 0.936774\n",
            "epoch: 2950, loss: 0.003294\n",
            "train accuracy: 0.925161\n",
            "epoch: 3000, loss: 0.002850\n",
            "train accuracy: 0.936129\n",
            "epoch: 3050, loss: 0.003002\n",
            "train accuracy: 0.937419\n",
            "epoch: 3100, loss: 0.003143\n",
            "train accuracy: 0.919355\n",
            "epoch: 3150, loss: 0.002608\n",
            "train accuracy: 0.920645\n",
            "epoch: 3200, loss: 0.004311\n",
            "train accuracy: 0.930968\n",
            "epoch: 3250, loss: 0.004076\n",
            "train accuracy: 0.929032\n",
            "epoch: 3300, loss: 0.003321\n",
            "train accuracy: 0.940645\n",
            "epoch: 3350, loss: 0.004324\n",
            "train accuracy: 0.927742\n",
            "epoch: 3400, loss: 0.004707\n",
            "train accuracy: 0.927097\n",
            "epoch: 3450, loss: 0.003096\n",
            "train accuracy: 0.934194\n",
            "epoch: 3500, loss: 0.003545\n",
            "train accuracy: 0.932903\n",
            "epoch: 3550, loss: 0.003764\n",
            "train accuracy: 0.931613\n",
            "epoch: 3600, loss: 0.003952\n",
            "train accuracy: 0.928387\n",
            "epoch: 3650, loss: 0.003890\n",
            "train accuracy: 0.927742\n",
            "epoch: 3700, loss: 0.003576\n",
            "train accuracy: 0.929677\n",
            "epoch: 3750, loss: 0.003152\n",
            "train accuracy: 0.932258\n",
            "epoch: 3800, loss: 0.003684\n",
            "train accuracy: 0.918065\n",
            "epoch: 3850, loss: 0.004085\n",
            "train accuracy: 0.931613\n",
            "epoch: 3900, loss: 0.003161\n",
            "train accuracy: 0.930968\n",
            "epoch: 3950, loss: 0.004720\n",
            "train accuracy: 0.927097\n",
            "epoch: 4000, loss: 0.003618\n",
            "train accuracy: 0.934839\n",
            "epoch: 4050, loss: 0.003402\n",
            "train accuracy: 0.932903\n",
            "epoch: 4100, loss: 0.003339\n",
            "train accuracy: 0.932258\n",
            "epoch: 4150, loss: 0.004287\n",
            "train accuracy: 0.924516\n",
            "epoch: 4200, loss: 0.003204\n",
            "train accuracy: 0.927097\n",
            "epoch: 4250, loss: 0.003999\n",
            "train accuracy: 0.934194\n",
            "epoch: 4300, loss: 0.002668\n",
            "train accuracy: 0.936129\n",
            "epoch: 4350, loss: 0.003023\n",
            "train accuracy: 0.930323\n",
            "epoch: 4400, loss: 0.003172\n",
            "train accuracy: 0.932258\n",
            "epoch: 4450, loss: 0.003557\n",
            "train accuracy: 0.933548\n",
            "epoch: 4500, loss: 0.003139\n",
            "train accuracy: 0.918065\n",
            "epoch: 4550, loss: 0.004070\n",
            "train accuracy: 0.929677\n",
            "epoch: 4600, loss: 0.003119\n",
            "train accuracy: 0.924516\n",
            "epoch: 4650, loss: 0.004034\n",
            "train accuracy: 0.928387\n",
            "epoch: 4700, loss: 0.002564\n",
            "train accuracy: 0.920645\n",
            "epoch: 4750, loss: 0.003438\n",
            "train accuracy: 0.933548\n",
            "epoch: 4800, loss: 0.003149\n",
            "train accuracy: 0.925806\n",
            "epoch: 4850, loss: 0.004369\n",
            "train accuracy: 0.922581\n",
            "epoch: 4900, loss: 0.003440\n",
            "train accuracy: 0.914839\n",
            "epoch: 4950, loss: 0.008653\n",
            "train accuracy: 0.922581\n",
            "epoch: 5000, loss: 0.003047\n",
            "train accuracy: 0.925161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "acc_sum = 0\n",
        "print(model(X_test).argmax(dim=1))\n",
        "print(Y_test.squeeze())\n",
        "acc_sum += (model(X_test).argmax(dim=1) == Y_test.squeeze()).sum()\n",
        "print(\"test accuracy: %f\" % (acc_sum/len(Y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPhwGbpoZRCi",
        "outputId": "6039fe14-607a-4f79-c461-acff27316bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3,  9,  6, 20, 30,  8, 23, 13, 29, 30, 12, 25, 18, 22, 26, 30, 17, 18,\n",
            "        19, 24, 23, 20, 23, 10, 13,  3,  2,  1,  2,  1,  5, 19, 17, 23,  4,  3,\n",
            "        12, 25, 10, 15, 30,  6,  7,  9, 10, 22,  6, 19, 27, 15, 11,  8, 12, 26,\n",
            "        24,  8, 28, 17,  6,  7,  1,  3,  9, 27, 18,  4,  6, 18, 19,  3, 23, 10,\n",
            "         5, 27, 18, 29, 18, 20, 29, 28,  0, 17,  6, 18, 15, 19,  4, 15, 16,  8,\n",
            "        23,  9, 11, 21,  4, 14, 15, 21,  1, 30, 16,  9,  3, 15, 14, 13, 19, 21,\n",
            "        14, 12, 16,  6,  6, 22,  9, 29, 25, 11, 22, 26, 15, 20, 12, 30,  7, 29,\n",
            "         6, 27, 15, 21, 12, 18, 29, 23, 17, 24,  6,  4,  5,  1,  2,  4, 17, 12,\n",
            "         2,  6, 18, 13, 27, 29, 30, 12, 19, 21, 11, 12,  8,  4, 24, 10,  2,  0,\n",
            "        28, 28, 14, 15, 24, 20, 24,  5, 15, 19, 12,  4, 17, 25, 13, 25, 11, 15,\n",
            "        26, 19,  3, 25, 14, 15,  5,  8,  8, 14,  2, 14, 23, 23, 20,  6,  1,  8,\n",
            "         7, 23, 16, 30, 14, 21, 11, 28, 30, 29, 20, 17,  6,  9,  1, 11, 30, 10,\n",
            "         1, 29, 10,  7,  1, 14, 16, 24, 26, 24, 27, 11, 13, 12, 10,  1,  5, 16,\n",
            "        20, 22, 15,  9, 23,  5, 28, 12,  8, 20, 20, 18, 20, 15, 24, 20,  2, 22,\n",
            "        30, 11, 10,  0, 14, 20, 28,  5, 14,  0,  5, 11, 12,  8, 13,  9, 17, 19,\n",
            "         4, 30, 21,  7, 28, 17, 10,  0,  9, 25,  9,  0, 18, 21, 16, 14,  8, 19,\n",
            "        27, 15,  8, 25, 30,  5, 21, 28, 21, 23, 27, 19, 18,  3, 12,  2, 11, 15,\n",
            "         6,  7, 19,  1,  9,  8, 23, 20, 10, 22, 23,  4, 15,  2, 16, 22,  5, 27,\n",
            "        18,  8, 28,  5,  2, 30, 18, 22, 12, 18,  5,  5, 22,  0, 25,  5,  8, 25,\n",
            "        22,  3, 16, 12, 18, 25, 24,  6, 13, 29, 17, 15, 24, 14, 22, 14, 26, 27,\n",
            "        25, 19, 28, 28, 17, 13, 12, 19, 10,  8, 20,  9,  1, 18, 21,  4, 12, 26,\n",
            "         3,  6, 18, 11,  6, 29,  9, 22,  8,  7,  0, 18, 27,  7, 23, 13, 18, 12,\n",
            "         1,  3, 22,  0, 25,  6, 19, 18, 23,  8, 19, 20, 24, 25, 13, 28,  1, 14,\n",
            "        21, 24, 28,  4,  1, 18,  4, 10, 22, 26, 19,  8,  6,  8,  3,  3, 28, 29,\n",
            "         3, 23, 21,  3,  9, 29,  7, 24, 13, 26, 24,  0, 28,  6,  2, 27, 20, 17,\n",
            "        21, 30, 10,  5, 30, 20, 29,  2, 10,  8, 30, 28, 25, 11, 27, 25,  8, 22,\n",
            "         1, 18, 10,  9, 19, 24, 27, 29, 30, 16, 20,  5, 12, 27, 15, 26, 27, 12,\n",
            "         4, 20,  8,  2,  4,  1,  7,  2, 19,  4, 14, 29, 24, 18,  7, 12,  6, 30,\n",
            "        16, 28, 22, 21, 23,  4,  7,  1, 29, 23, 24, 13, 19, 19,  9, 25, 23, 19,\n",
            "        25,  9,  5,  1,  2, 10,  3, 12, 14,  3,  4, 16, 25, 17,  4,  2, 17,  5,\n",
            "        17,  9, 18, 27,  0,  4, 25, 17,  9,  8, 10, 12,  6, 26, 24, 25,  9, 26,\n",
            "         7, 16,  6,  5, 11, 27, 25, 13, 23, 16, 23, 21, 22, 18, 10,  6, 22, 21,\n",
            "         1, 26, 28, 20, 21,  8, 22,  9,  2, 18, 13,  3, 20, 22, 14, 19,  5, 17,\n",
            "        21,  4, 10,  4, 27, 27, 12, 28, 20, 12, 13, 28, 12, 15, 23, 25, 25, 27,\n",
            "        10,  5,  9, 30, 17,  0,  7, 19])\n",
            "tensor([ 3,  9,  0, 20, 30,  8, 23, 13, 29, 30, 12, 25, 28, 13, 21, 26, 13, 28,\n",
            "        19, 24, 23, 20, 23, 10, 13,  3,  2,  1,  2,  1,  5, 19, 19, 23,  4,  3,\n",
            "        12, 25, 10, 15, 30,  6,  7,  9, 10, 22,  6, 19, 27, 15, 11,  8, 16, 26,\n",
            "        24,  8, 28, 17,  6,  7,  1,  3,  9, 27, 30,  4,  6, 18, 19,  3, 23, 10,\n",
            "         5, 27, 18, 29, 18, 20, 26, 28,  0, 17,  0, 18, 15, 19,  4, 15, 16,  2,\n",
            "        23,  9,  4, 21,  4, 14,  2, 21,  1, 30, 26,  9,  3, 15, 14, 13, 18, 21,\n",
            "        14, 12, 16,  6,  0, 22,  9, 29, 25, 11, 11,  2, 15, 30, 12,  7,  7, 29,\n",
            "         6, 27, 15, 21, 26, 18, 29, 23, 17, 24,  6,  4,  5,  1,  2, 15, 17, 12,\n",
            "         0,  6, 18, 13, 26, 29, 17, 12, 19, 27, 11, 12,  8,  4, 24, 10,  2,  0,\n",
            "        21, 28, 14, 15, 24, 20, 17,  5, 15, 19, 12,  4, 17, 29, 21, 25, 11, 15,\n",
            "        21, 19,  3, 25, 14, 15, 26, 16, 24, 14,  5, 14, 23, 23, 20,  6,  1,  8,\n",
            "         7, 16, 16, 30, 14, 21, 11, 28, 30, 29, 20, 17,  6, 11,  1, 11, 30, 10,\n",
            "         1, 29, 10,  0,  1, 14, 16, 24, 21, 24, 27, 11, 13, 12, 10,  1,  5, 16,\n",
            "        20, 22, 15,  9, 16,  5, 28, 11,  8, 20, 13,  7, 20, 15, 24, 20,  2, 13,\n",
            "        30, 11, 10,  0, 14, 20, 18,  5, 14,  0,  5, 11, 30,  8, 22,  9, 17, 13,\n",
            "         4, 30, 21,  7,  5, 17, 10,  0,  2, 25, 24,  0, 18, 21, 16, 14,  8, 19,\n",
            "        27, 15,  8, 25, 30,  5, 21, 26, 22, 23, 27, 26, 18,  3, 12,  2, 11, 15,\n",
            "         6,  0, 19,  1,  9,  8, 26, 20, 10, 27, 23, 24, 15,  2, 16, 22, 14, 27,\n",
            "         0,  8, 28,  5,  2, 30, 18, 22, 12, 30,  5, 28, 22,  0, 25,  5,  8, 26,\n",
            "        22,  3, 16, 11, 18, 25,  2,  6, 13, 29, 17, 15, 24, 14, 22, 14, 24, 27,\n",
            "        25, 11, 28, 28, 17, 13, 12, 19, 10,  8, 20,  9,  1,  7, 21,  4, 12, 26,\n",
            "         3,  6,  7, 11,  6, 29,  9, 22, 16,  7,  0,  7, 13,  7, 23, 13, 18, 16,\n",
            "         1,  3, 13,  0, 25,  6, 19, 18, 23,  8, 11, 20, 24, 25, 17, 16,  1, 14,\n",
            "        21, 26, 28,  4,  1,  5,  4, 22, 22, 26, 19,  8,  6,  8,  3,  3, 28, 29,\n",
            "         3, 23, 13,  3,  9, 29,  7, 24, 13, 26, 24,  0, 28,  6,  2, 27, 18, 17,\n",
            "        21,  7, 10,  5, 30, 20, 29,  2, 10,  8, 30, 28, 25, 11, 27, 25,  8, 22,\n",
            "         1,  7, 10,  9, 19, 24, 27, 29, 29, 16, 20,  5, 12, 13, 29, 30, 27, 12,\n",
            "         0, 20, 28,  2,  4,  1,  7,  2, 19,  4, 14, 15, 24, 18,  7, 12,  5, 30,\n",
            "        16, 28, 22, 22, 23,  4, 18,  1, 29, 23, 24, 13, 11, 19,  9, 25, 23, 19,\n",
            "        25,  9,  5,  1,  2, 10,  3, 12, 14,  3,  4, 16, 25, 17,  4,  2, 17,  5,\n",
            "        17,  9, 18, 26,  0,  4, 25, 17,  9,  8, 10, 12,  6, 29, 24, 23,  9, 26,\n",
            "         7, 16,  6, 14, 11, 27, 25, 13, 27, 16, 23, 21, 22,  7, 10,  6, 26, 21,\n",
            "         1, 26, 28, 20, 21,  8, 22,  9,  2, 18, 22,  3, 18, 22, 14, 19,  3, 17,\n",
            "        21,  4, 10,  4, 27, 27, 12, 28, 20, 12, 20, 28, 26, 15, 23, 25, 29, 27,\n",
            "        10,  3,  9, 30, 17,  0,  7, 19])\n",
            "test accuracy: 0.820968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_set = Data.TensorDataset(X_train, Y_train)\n",
        "train_loader = Data.DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.fc1 = nn.LSTM(20, 48)\n",
        "        self.fc2 = nn.Linear(48, 96)\n",
        "        self.fc3 = nn.Linear(96, 31)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x, _ = self.fc1(x.view(len(x), 1, -1))\n",
        "        x = self.fc2(x.view(len(x), 48))\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(self.fc3(x))\n",
        "        \n",
        "        return x\n",
        "    \n",
        "model = LSTM()"
      ],
      "metadata": {
        "id": "tIdpVAumanBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5000\n",
        "learning_rate = 1e-3\n",
        "batch_no = len(X_train) // batch_size\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Ju8NhA1Cts3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    loss_sum = 0\n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        y_pred = model(x)\n",
        "        y = y.squeeze()\n",
        "        loss = loss_function(y_pred, y)\n",
        "        loss_sum += loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 50 == 0:\n",
        "        print(\"epoch: %d, loss: %f\" % (epoch, loss_sum/batch_size))\n",
        "        acc_sum = 0\n",
        "        acc_sum += (model(X_train).argmax(dim=1) == Y_train.squeeze()).sum()\n",
        "        print(\"train accuracy: %f\" % (acc_sum/len(Y_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VNNNp5_twP_",
        "outputId": "514427b8-cdcb-4ece-91df-76938cacb5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 50, loss: 0.034463\n",
            "train accuracy: 0.685806\n",
            "epoch: 100, loss: 0.023148\n",
            "train accuracy: 0.745806\n",
            "epoch: 150, loss: 0.022472\n",
            "train accuracy: 0.785806\n",
            "epoch: 200, loss: 0.018702\n",
            "train accuracy: 0.807097\n",
            "epoch: 250, loss: 0.016048\n",
            "train accuracy: 0.807742\n",
            "epoch: 300, loss: 0.016648\n",
            "train accuracy: 0.823226\n",
            "epoch: 350, loss: 0.013059\n",
            "train accuracy: 0.832258\n",
            "epoch: 400, loss: 0.011482\n",
            "train accuracy: 0.850968\n",
            "epoch: 450, loss: 0.011480\n",
            "train accuracy: 0.862581\n",
            "epoch: 500, loss: 0.010533\n",
            "train accuracy: 0.863226\n",
            "epoch: 550, loss: 0.011325\n",
            "train accuracy: 0.858065\n",
            "epoch: 600, loss: 0.009258\n",
            "train accuracy: 0.875484\n",
            "epoch: 650, loss: 0.010269\n",
            "train accuracy: 0.872903\n",
            "epoch: 700, loss: 0.007596\n",
            "train accuracy: 0.890968\n",
            "epoch: 750, loss: 0.008104\n",
            "train accuracy: 0.905161\n",
            "epoch: 800, loss: 0.008029\n",
            "train accuracy: 0.897419\n",
            "epoch: 850, loss: 0.007929\n",
            "train accuracy: 0.901290\n",
            "epoch: 900, loss: 0.006231\n",
            "train accuracy: 0.897419\n",
            "epoch: 950, loss: 0.007057\n",
            "train accuracy: 0.909677\n",
            "epoch: 1000, loss: 0.006689\n",
            "train accuracy: 0.907097\n",
            "epoch: 1050, loss: 0.006525\n",
            "train accuracy: 0.920645\n",
            "epoch: 1100, loss: 0.005566\n",
            "train accuracy: 0.925161\n",
            "epoch: 1150, loss: 0.005431\n",
            "train accuracy: 0.922581\n",
            "epoch: 1200, loss: 0.004796\n",
            "train accuracy: 0.923226\n",
            "epoch: 1250, loss: 0.005100\n",
            "train accuracy: 0.918065\n",
            "epoch: 1300, loss: 0.005735\n",
            "train accuracy: 0.929677\n",
            "epoch: 1350, loss: 0.006182\n",
            "train accuracy: 0.914194\n",
            "epoch: 1400, loss: 0.005787\n",
            "train accuracy: 0.923871\n",
            "epoch: 1450, loss: 0.004612\n",
            "train accuracy: 0.929032\n",
            "epoch: 1500, loss: 0.005708\n",
            "train accuracy: 0.921935\n",
            "epoch: 1550, loss: 0.005102\n",
            "train accuracy: 0.939355\n",
            "epoch: 1600, loss: 0.004925\n",
            "train accuracy: 0.934839\n",
            "epoch: 1650, loss: 0.004850\n",
            "train accuracy: 0.934194\n",
            "epoch: 1700, loss: 0.004598\n",
            "train accuracy: 0.913548\n",
            "epoch: 1750, loss: 0.005157\n",
            "train accuracy: 0.918710\n",
            "epoch: 1800, loss: 0.004869\n",
            "train accuracy: 0.930323\n",
            "epoch: 1850, loss: 0.003965\n",
            "train accuracy: 0.921290\n",
            "epoch: 1900, loss: 0.004211\n",
            "train accuracy: 0.932258\n",
            "epoch: 1950, loss: 0.005026\n",
            "train accuracy: 0.924516\n",
            "epoch: 2000, loss: 0.003963\n",
            "train accuracy: 0.925161\n",
            "epoch: 2050, loss: 0.007298\n",
            "train accuracy: 0.926452\n",
            "epoch: 2100, loss: 0.003855\n",
            "train accuracy: 0.921935\n",
            "epoch: 2150, loss: 0.004281\n",
            "train accuracy: 0.932258\n",
            "epoch: 2200, loss: 0.004164\n",
            "train accuracy: 0.930968\n",
            "epoch: 2250, loss: 0.005211\n",
            "train accuracy: 0.936129\n",
            "epoch: 2300, loss: 0.004705\n",
            "train accuracy: 0.921290\n",
            "epoch: 2350, loss: 0.004078\n",
            "train accuracy: 0.922581\n",
            "epoch: 2400, loss: 0.004103\n",
            "train accuracy: 0.942581\n",
            "epoch: 2450, loss: 0.003333\n",
            "train accuracy: 0.929677\n",
            "epoch: 2500, loss: 0.006073\n",
            "train accuracy: 0.927097\n",
            "epoch: 2550, loss: 0.004549\n",
            "train accuracy: 0.925806\n",
            "epoch: 2600, loss: 0.004160\n",
            "train accuracy: 0.923226\n",
            "epoch: 2650, loss: 0.005062\n",
            "train accuracy: 0.929677\n",
            "epoch: 2700, loss: 0.003600\n",
            "train accuracy: 0.924516\n",
            "epoch: 2750, loss: 0.004205\n",
            "train accuracy: 0.920000\n",
            "epoch: 2800, loss: 0.003688\n",
            "train accuracy: 0.944516\n",
            "epoch: 2850, loss: 0.004038\n",
            "train accuracy: 0.926452\n",
            "epoch: 2900, loss: 0.004446\n",
            "train accuracy: 0.925161\n",
            "epoch: 2950, loss: 0.004225\n",
            "train accuracy: 0.935484\n",
            "epoch: 3000, loss: 0.004582\n",
            "train accuracy: 0.934839\n",
            "epoch: 3050, loss: 0.003112\n",
            "train accuracy: 0.927097\n",
            "epoch: 3100, loss: 0.004084\n",
            "train accuracy: 0.932258\n",
            "epoch: 3150, loss: 0.002897\n",
            "train accuracy: 0.929677\n",
            "epoch: 3200, loss: 0.004246\n",
            "train accuracy: 0.941935\n",
            "epoch: 3250, loss: 0.003616\n",
            "train accuracy: 0.930323\n",
            "epoch: 3300, loss: 0.003740\n",
            "train accuracy: 0.918065\n",
            "epoch: 3350, loss: 0.004068\n",
            "train accuracy: 0.936129\n",
            "epoch: 3400, loss: 0.003072\n",
            "train accuracy: 0.932258\n",
            "epoch: 3450, loss: 0.003840\n",
            "train accuracy: 0.933548\n",
            "epoch: 3500, loss: 0.003700\n",
            "train accuracy: 0.930323\n",
            "epoch: 3550, loss: 0.003740\n",
            "train accuracy: 0.938065\n",
            "epoch: 3600, loss: 0.003635\n",
            "train accuracy: 0.923871\n",
            "epoch: 3650, loss: 0.003588\n",
            "train accuracy: 0.929677\n",
            "epoch: 3700, loss: 0.002911\n",
            "train accuracy: 0.933548\n",
            "epoch: 3750, loss: 0.003475\n",
            "train accuracy: 0.921290\n",
            "epoch: 3800, loss: 0.003876\n",
            "train accuracy: 0.909032\n",
            "epoch: 3850, loss: 0.004840\n",
            "train accuracy: 0.919355\n",
            "epoch: 3900, loss: 0.003597\n",
            "train accuracy: 0.933548\n",
            "epoch: 3950, loss: 0.003910\n",
            "train accuracy: 0.927097\n",
            "epoch: 4000, loss: 0.004456\n",
            "train accuracy: 0.936129\n",
            "epoch: 4050, loss: 0.003477\n",
            "train accuracy: 0.931613\n",
            "epoch: 4100, loss: 0.003537\n",
            "train accuracy: 0.938710\n",
            "epoch: 4150, loss: 0.003806\n",
            "train accuracy: 0.938065\n",
            "epoch: 4200, loss: 0.003726\n",
            "train accuracy: 0.922581\n",
            "epoch: 4250, loss: 0.003966\n",
            "train accuracy: 0.929677\n",
            "epoch: 4300, loss: 0.003692\n",
            "train accuracy: 0.930323\n",
            "epoch: 4350, loss: 0.004413\n",
            "train accuracy: 0.932903\n",
            "epoch: 4400, loss: 0.003030\n",
            "train accuracy: 0.920645\n",
            "epoch: 4450, loss: 0.002763\n",
            "train accuracy: 0.940645\n",
            "epoch: 4500, loss: 0.003487\n",
            "train accuracy: 0.925806\n",
            "epoch: 4550, loss: 0.003787\n",
            "train accuracy: 0.920000\n",
            "epoch: 4600, loss: 0.003110\n",
            "train accuracy: 0.938710\n",
            "epoch: 4650, loss: 0.003026\n",
            "train accuracy: 0.930323\n",
            "epoch: 4700, loss: 0.003291\n",
            "train accuracy: 0.929677\n",
            "epoch: 4750, loss: 0.003045\n",
            "train accuracy: 0.936129\n",
            "epoch: 4800, loss: 0.003929\n",
            "train accuracy: 0.934839\n",
            "epoch: 4850, loss: 0.004445\n",
            "train accuracy: 0.930968\n",
            "epoch: 4900, loss: 0.003897\n",
            "train accuracy: 0.934194\n",
            "epoch: 4950, loss: 0.003229\n",
            "train accuracy: 0.923871\n",
            "epoch: 5000, loss: 0.002981\n",
            "train accuracy: 0.932903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "acc_sum = 0\n",
        "print(model(X_test).argmax(dim=1))\n",
        "print(Y_test.squeeze())\n",
        "acc_sum += (model(X_test.view(len(X_test), 1, -1)).argmax(dim=1) == Y_test.squeeze()).sum()\n",
        "print(\"test accuracy: %f\" % (acc_sum/len(Y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT54C4_wtyWI",
        "outputId": "9734bff0-5dc9-4bc5-d03d-fee4a6cee922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3,  9,  0, 20, 30,  8, 23, 13, 29, 13, 19, 25, 18, 24, 21, 30, 13, 18,\n",
            "        19, 24, 23, 20, 23, 10, 13,  3,  2,  1,  2,  1,  5, 19, 17, 23,  4,  3,\n",
            "        12, 25, 10, 15, 30,  6, 30,  9, 10, 22,  6, 19, 27, 15, 11,  8, 12, 26,\n",
            "        24,  8, 28, 17,  6,  7,  1,  3,  9, 27, 28,  4,  6, 18, 19,  3, 23, 10,\n",
            "         5, 27, 18, 29, 18, 20, 25, 28,  0, 17,  6, 18, 15, 19,  4, 15, 16, 24,\n",
            "        23,  9, 15, 21,  4, 14,  2, 21,  1, 30, 16,  9,  3, 15, 14, 13, 19, 21,\n",
            "        14, 12, 16,  6,  0, 17,  9, 29, 25, 11, 11,  2, 15, 20, 12, 30,  7, 29,\n",
            "         6, 27, 15, 21, 12, 18, 29, 23, 17, 24,  6,  4,  5,  1,  2,  4, 17, 12,\n",
            "         0,  6,  7, 13, 27, 29, 17, 12, 19, 27, 11, 12,  8,  4, 24, 10,  2,  0,\n",
            "        21, 28, 14, 15, 24, 20, 22,  5, 15, 19, 12,  4, 17, 25, 13, 25, 11, 15,\n",
            "        29, 19,  3, 25, 14, 15, 19, 16, 24, 14,  0, 14, 23, 23, 20,  5,  1,  8,\n",
            "         7, 23, 16, 30, 14, 21, 11, 28, 30, 29, 20, 17,  6, 28,  1, 11, 30, 10,\n",
            "         1, 29, 10,  0,  1, 14, 16, 24, 21, 24, 27, 11, 13, 12, 10,  1,  5, 16,\n",
            "        20, 22, 15,  9, 23,  5, 28, 12,  8, 20, 20, 28, 20, 15, 24, 20, 24, 13,\n",
            "        30, 11, 10,  0, 14, 20, 18,  5, 14,  0,  5, 11, 30,  8, 13,  9, 17, 19,\n",
            "         4, 30, 11,  7, 28, 17, 10,  0,  2, 25, 19,  0, 18, 21, 16, 14,  8, 19,\n",
            "        13, 15,  8, 25, 30,  5, 21, 28, 21, 23, 27, 19, 18,  3, 12,  2, 11, 15,\n",
            "         6,  7, 19,  1,  9,  8, 23, 20, 10, 21, 23, 19, 15, 24, 16, 22,  5, 27,\n",
            "         7,  8, 28,  5,  2, 30, 18, 22, 12, 18,  5,  7, 22,  0, 25,  5,  8, 25,\n",
            "        13,  3, 16, 28, 18, 25,  2,  6, 13, 29, 17, 15, 24, 14, 22, 14, 14, 27,\n",
            "        25, 11, 28, 28, 17, 13, 12, 19, 10,  8, 20,  9,  1, 20, 21,  4, 12, 26,\n",
            "         3,  6, 18, 11,  6, 29,  9, 22, 16,  7,  0, 18, 27,  7, 23, 13, 18, 12,\n",
            "         1,  3, 13,  0, 25,  6, 19, 18, 23,  8, 11, 20, 24, 25, 17, 28,  1, 14,\n",
            "        21, 24, 28,  4,  1, 28,  4, 22, 22, 26, 19,  8,  6,  8,  3,  3, 28, 11,\n",
            "         3, 23, 13,  3,  9, 29,  7, 24, 13, 26, 24,  0, 28,  6,  2, 27, 20, 17,\n",
            "        21,  7, 10,  5, 30, 20, 29,  2, 10,  8, 30, 28, 25, 11, 27, 25,  8, 22,\n",
            "         1, 18, 10,  9, 22, 24, 27, 29, 30, 16, 20,  5, 12, 27, 15, 26, 27, 12,\n",
            "         0, 20, 28, 24,  4,  1,  7,  2, 19,  4, 14, 29, 24,  7,  7, 26,  5, 30,\n",
            "        16, 28, 22, 21, 23,  4,  7,  1, 29, 23, 24, 13, 28, 19,  9, 25, 23, 19,\n",
            "        25,  9,  5,  1,  2, 10,  3, 12, 14,  3,  4, 16, 25, 22,  4,  2, 17,  5,\n",
            "        22,  9, 28,  5,  0,  4, 25, 17,  9,  8, 10, 12,  6, 26, 24, 23,  9, 12,\n",
            "         7, 16,  6,  0,  4, 27, 25, 13, 26, 16, 23, 21, 22, 18, 10,  6, 19, 21,\n",
            "         1, 26, 28, 20, 21,  8, 22,  9,  2, 18, 13,  3, 20, 22, 14, 19,  5, 17,\n",
            "        21,  4, 10,  4, 27, 27, 12, 28, 20, 12, 13, 28, 12, 15, 23, 25, 29, 27,\n",
            "        10,  3,  9, 30, 17,  0,  7, 19])\n",
            "tensor([ 3,  9,  0, 20, 30,  8, 23, 13, 29, 30, 12, 25, 28, 13, 21, 26, 13, 28,\n",
            "        19, 24, 23, 20, 23, 10, 13,  3,  2,  1,  2,  1,  5, 19, 19, 23,  4,  3,\n",
            "        12, 25, 10, 15, 30,  6,  7,  9, 10, 22,  6, 19, 27, 15, 11,  8, 16, 26,\n",
            "        24,  8, 28, 17,  6,  7,  1,  3,  9, 27, 30,  4,  6, 18, 19,  3, 23, 10,\n",
            "         5, 27, 18, 29, 18, 20, 26, 28,  0, 17,  0, 18, 15, 19,  4, 15, 16,  2,\n",
            "        23,  9,  4, 21,  4, 14,  2, 21,  1, 30, 26,  9,  3, 15, 14, 13, 18, 21,\n",
            "        14, 12, 16,  6,  0, 22,  9, 29, 25, 11, 11,  2, 15, 30, 12,  7,  7, 29,\n",
            "         6, 27, 15, 21, 26, 18, 29, 23, 17, 24,  6,  4,  5,  1,  2, 15, 17, 12,\n",
            "         0,  6, 18, 13, 26, 29, 17, 12, 19, 27, 11, 12,  8,  4, 24, 10,  2,  0,\n",
            "        21, 28, 14, 15, 24, 20, 17,  5, 15, 19, 12,  4, 17, 29, 21, 25, 11, 15,\n",
            "        21, 19,  3, 25, 14, 15, 26, 16, 24, 14,  5, 14, 23, 23, 20,  6,  1,  8,\n",
            "         7, 16, 16, 30, 14, 21, 11, 28, 30, 29, 20, 17,  6, 11,  1, 11, 30, 10,\n",
            "         1, 29, 10,  0,  1, 14, 16, 24, 21, 24, 27, 11, 13, 12, 10,  1,  5, 16,\n",
            "        20, 22, 15,  9, 16,  5, 28, 11,  8, 20, 13,  7, 20, 15, 24, 20,  2, 13,\n",
            "        30, 11, 10,  0, 14, 20, 18,  5, 14,  0,  5, 11, 30,  8, 22,  9, 17, 13,\n",
            "         4, 30, 21,  7,  5, 17, 10,  0,  2, 25, 24,  0, 18, 21, 16, 14,  8, 19,\n",
            "        27, 15,  8, 25, 30,  5, 21, 26, 22, 23, 27, 26, 18,  3, 12,  2, 11, 15,\n",
            "         6,  0, 19,  1,  9,  8, 26, 20, 10, 27, 23, 24, 15,  2, 16, 22, 14, 27,\n",
            "         0,  8, 28,  5,  2, 30, 18, 22, 12, 30,  5, 28, 22,  0, 25,  5,  8, 26,\n",
            "        22,  3, 16, 11, 18, 25,  2,  6, 13, 29, 17, 15, 24, 14, 22, 14, 24, 27,\n",
            "        25, 11, 28, 28, 17, 13, 12, 19, 10,  8, 20,  9,  1,  7, 21,  4, 12, 26,\n",
            "         3,  6,  7, 11,  6, 29,  9, 22, 16,  7,  0,  7, 13,  7, 23, 13, 18, 16,\n",
            "         1,  3, 13,  0, 25,  6, 19, 18, 23,  8, 11, 20, 24, 25, 17, 16,  1, 14,\n",
            "        21, 26, 28,  4,  1,  5,  4, 22, 22, 26, 19,  8,  6,  8,  3,  3, 28, 29,\n",
            "         3, 23, 13,  3,  9, 29,  7, 24, 13, 26, 24,  0, 28,  6,  2, 27, 18, 17,\n",
            "        21,  7, 10,  5, 30, 20, 29,  2, 10,  8, 30, 28, 25, 11, 27, 25,  8, 22,\n",
            "         1,  7, 10,  9, 19, 24, 27, 29, 29, 16, 20,  5, 12, 13, 29, 30, 27, 12,\n",
            "         0, 20, 28,  2,  4,  1,  7,  2, 19,  4, 14, 15, 24, 18,  7, 12,  5, 30,\n",
            "        16, 28, 22, 22, 23,  4, 18,  1, 29, 23, 24, 13, 11, 19,  9, 25, 23, 19,\n",
            "        25,  9,  5,  1,  2, 10,  3, 12, 14,  3,  4, 16, 25, 17,  4,  2, 17,  5,\n",
            "        17,  9, 18, 26,  0,  4, 25, 17,  9,  8, 10, 12,  6, 29, 24, 23,  9, 26,\n",
            "         7, 16,  6, 14, 11, 27, 25, 13, 27, 16, 23, 21, 22,  7, 10,  6, 26, 21,\n",
            "         1, 26, 28, 20, 21,  8, 22,  9,  2, 18, 22,  3, 18, 22, 14, 19,  3, 17,\n",
            "        21,  4, 10,  4, 27, 27, 12, 28, 20, 12, 20, 28, 26, 15, 23, 25, 29, 27,\n",
            "        10,  3,  9, 30, 17,  0,  7, 19])\n",
            "test accuracy: 0.841935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = pd.read_csv('./train_open.csv')\n",
        "testset = pd.read_csv('./test_open.csv')\n",
        "\n",
        "print(trainset)\n",
        "print(testset)\n",
        "\n",
        "X_train = trainset.iloc[:, 0:20].values\n",
        "Y_train = trainset.iloc[:, 20].values\n",
        "\n",
        "X_test = testset.iloc[:, 0:20].values\n",
        "Y_test = testset.iloc[:, 20].values\n",
        "\n",
        "X_train = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
        "Y_train = torch.from_numpy(Y_train).type(torch.LongTensor)\n",
        "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
        "Y_test = torch.from_numpy(Y_test).type(torch.LongTensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOqiCT7lztYp",
        "outputId": "acb62c05-d10a-4258-bbfd-1c17c663736c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      cache-misses-1  node-loads-1  branch-misses-1  branch-load-misses-1  \\\n",
            "0           36218220       2530196         29315596              29316371   \n",
            "1           34025893       2310966         29030928              29031111   \n",
            "2           36778380       2597194         32242092              32242655   \n",
            "3           27574965       2081723         18248848              18249034   \n",
            "4           35736215       2499164         32035503              32038530   \n",
            "...              ...           ...              ...                   ...   \n",
            "1545        32765499       2249790         28748154              28751282   \n",
            "1546        36032964       2675011         23717334              23717871   \n",
            "1547        34176184       2447644         23153779              23154422   \n",
            "1548        32203221       2189984         28521743              28523470   \n",
            "1549        34253061       2361245         30368723              30370088   \n",
            "\n",
            "      cache-misses-2  node-loads-2  branch-misses-2  branch-load-misses-2  \\\n",
            "0           67010880       4826768         44219557              44218791   \n",
            "1           74840153       5778039         49779543              49780197   \n",
            "2           74405269       5584753         47564426              47564998   \n",
            "3           73767558       5803860         43940208              43940463   \n",
            "4           65822099       4772384         41975936              41972911   \n",
            "...              ...           ...              ...                   ...   \n",
            "1545        82592348       6463990         52916431              52915731   \n",
            "1546        77930191       6112430         47922023              47923542   \n",
            "1547        79152462       6068994         45784778              45784503   \n",
            "1548        79274149       6177813         52673555              52673172   \n",
            "1549        79908261       6256067         51440874              51441498   \n",
            "\n",
            "      cache-misses-3  node-loads-3  ...  branch-load-misses-3  cache-misses-4  \\\n",
            "0           61089052       4770075  ...              33063440        58901471   \n",
            "1           84125769       6507363  ...              41147892        47180922   \n",
            "2           64467520       4966192  ...              38899553        58727157   \n",
            "3           72955123       5991063  ...              32526988        59468415   \n",
            "4           64615266       4967699  ...              37184610        69232206   \n",
            "...              ...           ...  ...                   ...             ...   \n",
            "1545        69972572       5276739  ...              36389062        64962841   \n",
            "1546        73733629       6031949  ...              32944803        50344665   \n",
            "1547        77430746       6307641  ...              38594648        46941364   \n",
            "1548        75339650       5781138  ...              40070212        57134701   \n",
            "1549        69103527       5103737  ...              41377859        49707898   \n",
            "\n",
            "      node-loads-4  branch-misses-4  branch-load-misses-4  cache-misses-5  \\\n",
            "0          4235577         23518737              23517769        42228033   \n",
            "1          3188601         22359608              22358975        53289344   \n",
            "2          4140510         26356572              26357154        47354346   \n",
            "3          4568843         32546299              32547098        68949748   \n",
            "4          4963483         30828355              30828973        51816377   \n",
            "...            ...              ...                   ...             ...   \n",
            "1545       4761708         33874589              33867266        47447789   \n",
            "1546       3540111         25981418              25980070        46326190   \n",
            "1547       3405554         24044938              24043318        46725654   \n",
            "1548       4050827         32791802              32790501        48657736   \n",
            "1549       3414122         21085976              21085055        48424225   \n",
            "\n",
            "      node-loads-5  branch-misses-5  branch-load-misses-5  label  \n",
            "0          2981700         17816182              17816426      7  \n",
            "1          3827425         38928056              38927513     23  \n",
            "2          3340845         18700359              18699857     17  \n",
            "3          4417253         58589561              58588155      0  \n",
            "4          3675632         19138978              19137869     12  \n",
            "...            ...              ...                   ...    ...  \n",
            "1545       3374897         16673913              16673701     26  \n",
            "1546       3217768         16989584              16989614      1  \n",
            "1547       3306579         17000690              17000473      1  \n",
            "1548       3434079         17285141              17285080     26  \n",
            "1549       3449959         17671119              17671581     13  \n",
            "\n",
            "[1550 rows x 21 columns]\n",
            "     cache-misses-1  node-loads-1  branch-misses-1  branch-load-misses-1  \\\n",
            "0          33534639       2368872         22796799              22797179   \n",
            "1          32232524       2183296         28148918              28150145   \n",
            "2          26970152       1962052         17273316              17273348   \n",
            "3          33022157       2301241         27949457              27950095   \n",
            "4          30530041       2055189         25497203              25497316   \n",
            "..              ...           ...              ...                   ...   \n",
            "615        38171975       2742669         32189665              32191087   \n",
            "616        32279778       2202587         26713082              26714179   \n",
            "617        25598187       1914919         16633090              16633360   \n",
            "618        35783099       2524731         30651568              30652079   \n",
            "619        33492002       2357476         29188532              29189261   \n",
            "\n",
            "     cache-misses-2  node-loads-2  branch-misses-2  branch-load-misses-2  \\\n",
            "0          70491405       5571363         45438807              45439991   \n",
            "1          76925940       5729503         49921203              49920450   \n",
            "2          66149906       5036740         41494999              41495228   \n",
            "3          67314209       4813251         44448154              44447519   \n",
            "4          73335409       5329356         50222727              50223219   \n",
            "..              ...           ...              ...                   ...   \n",
            "615        70420773       5124793         46039463              46040717   \n",
            "616        81179111       6096084         52939317              52939088   \n",
            "617        66324639       5136472         42346187              42347119   \n",
            "618        64756446       4622112         40832170              40831659   \n",
            "619        68250729       4971700         44577279              44576650   \n",
            "\n",
            "     cache-misses-3  node-loads-3  ...  branch-load-misses-3  cache-misses-4  \\\n",
            "0          63848650       4933524  ...              32622917        78797245   \n",
            "1          69055005       5292372  ...              39405615        59060107   \n",
            "2          72247835       5830495  ...              36866418        71769005   \n",
            "3          42632597       2983742  ...              20771243        52042219   \n",
            "4          78397977       6121909  ...              43882288        74406504   \n",
            "..              ...           ...  ...                   ...             ...   \n",
            "615        76377171       5917658  ...              43122648        72545211   \n",
            "616        62124743       4673479  ...              40138287        58390361   \n",
            "617        79027933       6564622  ...              36322218        66702187   \n",
            "618        51721879       3779745  ...              27685264        59953751   \n",
            "619        64075211       4991918  ...              38925833        59626039   \n",
            "\n",
            "     node-loads-4  branch-misses-4  branch-load-misses-4  cache-misses-5  \\\n",
            "0         5766023         35649588              35651276        72198012   \n",
            "1         4121234         31725687              31724672        56674876   \n",
            "2         6094217         28593327              28595525        97611968   \n",
            "3         3838474         22377095              22377033        33993195   \n",
            "4         5196670         37276931              37276481        47815700   \n",
            "..            ...              ...                   ...             ...   \n",
            "615       5173016         33562165              33559599        47452445   \n",
            "616       4150812         27847639              27850221        44855595   \n",
            "617       5313780         31228163              31228906        80462464   \n",
            "618       4441445         28156119              28155955        41649164   \n",
            "619       4203048         26005460              26003860        50433679   \n",
            "\n",
            "     node-loads-5  branch-misses-5  branch-load-misses-5  label  \n",
            "0         5515362         22353340              22351242      3  \n",
            "1         3974914         24357365              24357368      9  \n",
            "2         7736877         42193461              42191184      0  \n",
            "3         2646994         16859897              16859615     20  \n",
            "4         3309081         17478460              17478380     30  \n",
            "..            ...              ...                   ...    ...  \n",
            "615       3276112         17276394              17275995     30  \n",
            "616       3139808         17592958              17590848     17  \n",
            "617       6363729         26995259              26994285      0  \n",
            "618       3224799         19006770              19007292      7  \n",
            "619       3491833         34301570              34302105     19  \n",
            "\n",
            "[620 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "train_set = Data.TensorDataset(X_train, Y_train)\n",
        "train_loader = Data.DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "model = MLP()\n",
        "\n",
        "num_epochs = 5000\n",
        "learning_rate = 1e-4\n",
        "batch_no = len(X_train) // batch_size\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "TzIYsluVE3Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    loss_sum = 0\n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        y_pred = model(x)\n",
        "        y = y.squeeze()\n",
        "        loss = loss_function(y_pred, y)\n",
        "        loss_sum += loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 50 == 0:\n",
        "        print(\"epoch: %d, loss: %f\" % (epoch, loss_sum/batch_size))\n",
        "        acc_sum = 0\n",
        "        acc_sum += (model(X_train).argmax(dim=1) == Y_train.squeeze()).sum()\n",
        "        print(\"train accuracy: %f\" % (acc_sum/len(Y_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etVGYoQHE_xo",
        "outputId": "3a6616c4-2d09-45b9-9862-173f1dde2464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 50, loss: 959.498779\n",
            "train accuracy: 0.185806\n",
            "epoch: 100, loss: 436.590515\n",
            "train accuracy: 0.344516\n",
            "epoch: 150, loss: 254.286652\n",
            "train accuracy: 0.409032\n",
            "epoch: 200, loss: 248.000259\n",
            "train accuracy: 0.418065\n",
            "epoch: 250, loss: 148.978638\n",
            "train accuracy: 0.484516\n",
            "epoch: 300, loss: 140.730072\n",
            "train accuracy: 0.519355\n",
            "epoch: 350, loss: 120.782654\n",
            "train accuracy: 0.525806\n",
            "epoch: 400, loss: 91.838745\n",
            "train accuracy: 0.550968\n",
            "epoch: 450, loss: 89.856956\n",
            "train accuracy: 0.591613\n",
            "epoch: 500, loss: 139.464264\n",
            "train accuracy: 0.556774\n",
            "epoch: 550, loss: 73.121948\n",
            "train accuracy: 0.567742\n",
            "epoch: 600, loss: 72.751381\n",
            "train accuracy: 0.592903\n",
            "epoch: 650, loss: 91.708054\n",
            "train accuracy: 0.578065\n",
            "epoch: 700, loss: 83.268364\n",
            "train accuracy: 0.664516\n",
            "epoch: 750, loss: 90.647385\n",
            "train accuracy: 0.620645\n",
            "epoch: 800, loss: 65.534286\n",
            "train accuracy: 0.629677\n",
            "epoch: 850, loss: 56.716789\n",
            "train accuracy: 0.633548\n",
            "epoch: 900, loss: 67.574806\n",
            "train accuracy: 0.622581\n",
            "epoch: 950, loss: 58.306499\n",
            "train accuracy: 0.654839\n",
            "epoch: 1000, loss: 66.115509\n",
            "train accuracy: 0.625161\n",
            "epoch: 1050, loss: 83.362885\n",
            "train accuracy: 0.624516\n",
            "epoch: 1100, loss: 52.600498\n",
            "train accuracy: 0.665161\n",
            "epoch: 1150, loss: 50.166962\n",
            "train accuracy: 0.680645\n",
            "epoch: 1200, loss: 43.048660\n",
            "train accuracy: 0.689032\n",
            "epoch: 1250, loss: 44.336140\n",
            "train accuracy: 0.689032\n",
            "epoch: 1300, loss: 86.795380\n",
            "train accuracy: 0.631613\n",
            "epoch: 1350, loss: 41.390476\n",
            "train accuracy: 0.694839\n",
            "epoch: 1400, loss: 32.794743\n",
            "train accuracy: 0.654194\n",
            "epoch: 1450, loss: 57.102707\n",
            "train accuracy: 0.677419\n",
            "epoch: 1500, loss: 83.035461\n",
            "train accuracy: 0.645806\n",
            "epoch: 1550, loss: 51.341873\n",
            "train accuracy: 0.672258\n",
            "epoch: 1600, loss: 28.081089\n",
            "train accuracy: 0.712258\n",
            "epoch: 1650, loss: 39.006210\n",
            "train accuracy: 0.694194\n",
            "epoch: 1700, loss: 34.551582\n",
            "train accuracy: 0.702581\n",
            "epoch: 1750, loss: 41.140545\n",
            "train accuracy: 0.721936\n",
            "epoch: 1800, loss: 39.065754\n",
            "train accuracy: 0.697419\n",
            "epoch: 1850, loss: 81.398254\n",
            "train accuracy: 0.656129\n",
            "epoch: 1900, loss: 28.342484\n",
            "train accuracy: 0.704516\n",
            "epoch: 1950, loss: 33.599167\n",
            "train accuracy: 0.694194\n",
            "epoch: 2000, loss: 47.120476\n",
            "train accuracy: 0.698710\n",
            "epoch: 2050, loss: 24.179470\n",
            "train accuracy: 0.746452\n",
            "epoch: 2100, loss: 29.816204\n",
            "train accuracy: 0.762581\n",
            "epoch: 2150, loss: 41.412003\n",
            "train accuracy: 0.718064\n",
            "epoch: 2200, loss: 36.810303\n",
            "train accuracy: 0.723226\n",
            "epoch: 2250, loss: 47.330849\n",
            "train accuracy: 0.718064\n",
            "epoch: 2300, loss: 40.914772\n",
            "train accuracy: 0.697419\n",
            "epoch: 2350, loss: 46.363037\n",
            "train accuracy: 0.740000\n",
            "epoch: 2400, loss: 26.268990\n",
            "train accuracy: 0.685806\n",
            "epoch: 2450, loss: 20.396696\n",
            "train accuracy: 0.736129\n",
            "epoch: 2500, loss: 23.177628\n",
            "train accuracy: 0.781290\n",
            "epoch: 2550, loss: 23.404541\n",
            "train accuracy: 0.727742\n",
            "epoch: 2600, loss: 42.775711\n",
            "train accuracy: 0.759355\n",
            "epoch: 2650, loss: 33.752460\n",
            "train accuracy: 0.770323\n",
            "epoch: 2700, loss: 42.922295\n",
            "train accuracy: 0.689032\n",
            "epoch: 2750, loss: 24.419580\n",
            "train accuracy: 0.718710\n",
            "epoch: 2800, loss: 32.668182\n",
            "train accuracy: 0.723226\n",
            "epoch: 2850, loss: 16.873978\n",
            "train accuracy: 0.798065\n",
            "epoch: 2900, loss: 16.741838\n",
            "train accuracy: 0.760000\n",
            "epoch: 2950, loss: 20.684555\n",
            "train accuracy: 0.763871\n",
            "epoch: 3000, loss: 17.295937\n",
            "train accuracy: 0.756129\n",
            "epoch: 3050, loss: 16.640188\n",
            "train accuracy: 0.760645\n",
            "epoch: 3100, loss: 23.905756\n",
            "train accuracy: 0.750968\n",
            "epoch: 3150, loss: 30.189625\n",
            "train accuracy: 0.722581\n",
            "epoch: 3200, loss: 37.212585\n",
            "train accuracy: 0.745161\n",
            "epoch: 3250, loss: 21.818155\n",
            "train accuracy: 0.744516\n",
            "epoch: 3300, loss: 39.472847\n",
            "train accuracy: 0.734839\n",
            "epoch: 3350, loss: 22.737963\n",
            "train accuracy: 0.706452\n",
            "epoch: 3400, loss: 24.513332\n",
            "train accuracy: 0.749677\n",
            "epoch: 3450, loss: 26.267292\n",
            "train accuracy: 0.769032\n",
            "epoch: 3500, loss: 12.884793\n",
            "train accuracy: 0.798065\n",
            "epoch: 3550, loss: 19.404251\n",
            "train accuracy: 0.769032\n",
            "epoch: 3600, loss: 24.517769\n",
            "train accuracy: 0.765161\n",
            "epoch: 3650, loss: 15.746728\n",
            "train accuracy: 0.792258\n",
            "epoch: 3700, loss: 23.698105\n",
            "train accuracy: 0.725161\n",
            "epoch: 3750, loss: 25.244495\n",
            "train accuracy: 0.774194\n",
            "epoch: 3800, loss: 13.718356\n",
            "train accuracy: 0.823226\n",
            "epoch: 3850, loss: 10.556277\n",
            "train accuracy: 0.776129\n",
            "epoch: 3900, loss: 11.273515\n",
            "train accuracy: 0.765161\n",
            "epoch: 3950, loss: 15.902445\n",
            "train accuracy: 0.781290\n",
            "epoch: 4000, loss: 12.868119\n",
            "train accuracy: 0.784516\n",
            "epoch: 4050, loss: 12.978550\n",
            "train accuracy: 0.816129\n",
            "epoch: 4100, loss: 29.519836\n",
            "train accuracy: 0.772258\n",
            "epoch: 4150, loss: 15.547264\n",
            "train accuracy: 0.805806\n",
            "epoch: 4200, loss: 20.164692\n",
            "train accuracy: 0.815484\n",
            "epoch: 4250, loss: 9.686331\n",
            "train accuracy: 0.822581\n",
            "epoch: 4300, loss: 11.062811\n",
            "train accuracy: 0.778064\n",
            "epoch: 4350, loss: 27.098751\n",
            "train accuracy: 0.782581\n",
            "epoch: 4400, loss: 13.249115\n",
            "train accuracy: 0.742581\n",
            "epoch: 4450, loss: 21.404747\n",
            "train accuracy: 0.750323\n",
            "epoch: 4500, loss: 33.063953\n",
            "train accuracy: 0.763226\n",
            "epoch: 4550, loss: 5.631279\n",
            "train accuracy: 0.827742\n",
            "epoch: 4600, loss: 19.723183\n",
            "train accuracy: 0.814839\n",
            "epoch: 4650, loss: 24.004688\n",
            "train accuracy: 0.783871\n",
            "epoch: 4700, loss: 14.422746\n",
            "train accuracy: 0.816774\n",
            "epoch: 4750, loss: 17.063713\n",
            "train accuracy: 0.816129\n",
            "epoch: 4800, loss: 12.175957\n",
            "train accuracy: 0.814839\n",
            "epoch: 4850, loss: 23.279776\n",
            "train accuracy: 0.754839\n",
            "epoch: 4900, loss: 15.014521\n",
            "train accuracy: 0.808387\n",
            "epoch: 4950, loss: 21.091379\n",
            "train accuracy: 0.780645\n",
            "epoch: 5000, loss: 14.509741\n",
            "train accuracy: 0.858065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "acc_sum = 0\n",
        "print(model(X_test).argmax(dim=1))\n",
        "print(Y_test.squeeze())\n",
        "acc_sum += (model(X_test).argmax(dim=1) == Y_test.squeeze()).sum()\n",
        "print(\"test accuracy: %f\" % (acc_sum/len(Y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB5Yj5beFAw5",
        "outputId": "4798abed-c00b-4e61-ceed-08fdcae5a67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3,  9,  6, 20, 25,  8, 23, 13, 29, 30, 12, 25, 26, 22, 13, 30, 13, 28,\n",
            "        19, 24, 25, 20, 23, 10, 13,  3,  2,  1,  2,  1,  5, 19, 17, 23,  4,  3,\n",
            "        12, 25, 10, 15, 30,  6, 30, 29, 10, 22,  6, 19, 27, 15, 28, 28, 12, 26,\n",
            "        24,  8, 28, 17,  6,  7,  1,  3, 11, 22, 28,  4,  6, 18, 19,  3, 23, 10,\n",
            "         5, 13, 18, 29, 28, 20, 15, 28,  0, 17,  4, 18,  2, 19,  4, 15, 16, 28,\n",
            "        23, 29, 15, 21,  4, 14,  2, 21,  1, 30, 16, 29,  3, 15, 12, 13,  7, 21,\n",
            "        28, 16, 16,  6,  6, 22,  9, 29, 25, 11, 22, 21, 29, 20, 12,  7,  7, 29,\n",
            "         6, 27, 15, 21, 13, 18, 29, 23, 17, 25,  6,  4,  5,  1,  2,  4, 17, 12,\n",
            "         0,  6,  7, 13, 27, 29, 22, 12, 19, 21, 11, 12,  8,  4, 24, 10,  2,  0,\n",
            "        21, 28, 14, 15,  5, 20, 25,  5, 15, 19, 12,  4, 17, 29, 13, 25, 11, 15,\n",
            "        29, 19,  3, 25, 21, 15,  7, 16, 24, 28,  4, 14, 23, 23, 20,  6,  1,  8,\n",
            "         7, 23, 16, 22, 14, 21, 19, 28,  5,  9, 20, 17,  6, 28,  1, 11, 30, 10,\n",
            "         1, 29, 10,  7,  1,  7, 16, 24, 21, 24, 27, 11, 13, 12, 10,  1,  5, 16,\n",
            "        20, 22, 15, 21, 23,  5, 28, 19, 28, 18, 20, 18, 20,  2, 24, 20,  2, 22,\n",
            "        30, 21, 10,  0, 14, 20, 18,  5, 14, 27,  5, 11, 30,  8, 13,  9, 17,  7,\n",
            "         4, 30, 21,  7,  0, 17, 10,  0, 29, 25, 18,  0, 18, 21, 16, 14, 28, 19,\n",
            "        13, 15,  8, 25, 30,  5, 21, 28, 21, 23, 27, 17, 18,  3, 16,  2, 11, 15,\n",
            "         6,  7, 19,  1,  9,  8, 23, 20, 10, 22, 23, 22,  4,  2, 16, 22, 14, 27,\n",
            "         7,  8, 28,  5,  2, 30, 18, 22, 12, 28,  5, 28, 22,  0, 25,  5,  8, 23,\n",
            "        13,  3, 16, 28,  5, 25,  2,  6, 13, 29, 17, 15, 24, 14, 22, 14, 14, 27,\n",
            "        25,  8, 28, 28, 17, 13, 12, 19, 10,  8, 20,  9,  1, 18, 21, 24, 12, 30,\n",
            "         3,  6, 18, 11,  6, 30,  9, 22,  8,  7,  5, 18, 13,  7, 23, 13, 18, 16,\n",
            "         1,  3, 22,  0, 25,  6, 19, 18, 23,  8, 19, 20, 24, 25, 17,  8,  1, 14,\n",
            "        21, 26, 28,  4,  1,  7,  4,  7, 22, 21, 19,  8,  6,  8,  3,  3, 28, 29,\n",
            "         3, 23, 13,  3, 17,  9,  7, 12, 13, 21, 24,  0, 28,  6, 21, 27, 20, 17,\n",
            "        21,  7, 10,  5, 30, 20, 29,  2, 10,  8, 30, 18, 25, 11, 27, 25,  8, 22,\n",
            "         1, 18, 10,  9, 22, 24, 27, 29, 29, 16, 20,  3, 12, 13, 15, 30, 27, 12,\n",
            "         4, 20,  8,  2,  4,  1,  7,  2, 19,  4, 14, 15, 24, 18,  7, 12,  5, 30,\n",
            "        16, 28, 10, 22, 23,  4,  7,  1, 29, 23, 24, 13, 19, 19,  9, 25, 23, 19,\n",
            "        25,  9,  5,  1,  2, 10,  3, 16, 26,  3,  4, 16, 25, 17,  4,  2, 17,  5,\n",
            "        17, 26, 28, 13,  0,  4, 25, 17, 21, 28, 10, 21,  6, 29, 24, 23, 21, 12,\n",
            "         7, 16,  6,  4, 11, 29, 25, 13, 21, 16, 23, 21, 17, 18, 10,  6, 18, 21,\n",
            "         1, 12, 28, 20, 21,  8, 21,  9,  9, 19, 13,  3, 20, 22,  7, 19,  3, 17,\n",
            "        21,  4, 10,  4, 27, 27, 12, 28, 20, 30, 20, 28, 12, 25, 23, 25, 29, 27,\n",
            "        10,  3, 21, 30, 17,  0,  7, 19])\n",
            "tensor([ 3,  9,  0, 20, 30,  8, 23, 13, 29, 30, 12, 25, 28, 13, 21, 26, 13, 28,\n",
            "        19, 24, 23, 20, 23, 10, 13,  3,  2,  1,  2,  1,  5, 19, 19, 23,  4,  3,\n",
            "        12, 25, 10, 15, 30,  6,  7,  9, 10, 22,  6, 19, 27, 15, 11,  8, 16, 26,\n",
            "        24,  8, 28, 17,  6,  7,  1,  3,  9, 27, 30,  4,  6, 18, 19,  3, 23, 10,\n",
            "         5, 27, 18, 29, 18, 20, 26, 28,  0, 17,  0, 18, 15, 19,  4, 15, 16,  2,\n",
            "        23,  9,  4, 21,  4, 14,  2, 21,  1, 30, 26,  9,  3, 15, 14, 13, 18, 21,\n",
            "        14, 12, 16,  6,  0, 22,  9, 29, 25, 11, 11,  2, 15, 30, 12,  7,  7, 29,\n",
            "         6, 27, 15, 21, 26, 18, 29, 23, 17, 24,  6,  4,  5,  1,  2, 15, 17, 12,\n",
            "         0,  6, 18, 13, 26, 29, 17, 12, 19, 27, 11, 12,  8,  4, 24, 10,  2,  0,\n",
            "        21, 28, 14, 15, 24, 20, 17,  5, 15, 19, 12,  4, 17, 29, 21, 25, 11, 15,\n",
            "        21, 19,  3, 25, 14, 15, 26, 16, 24, 14,  5, 14, 23, 23, 20,  6,  1,  8,\n",
            "         7, 16, 16, 30, 14, 21, 11, 28, 30, 29, 20, 17,  6, 11,  1, 11, 30, 10,\n",
            "         1, 29, 10,  0,  1, 14, 16, 24, 21, 24, 27, 11, 13, 12, 10,  1,  5, 16,\n",
            "        20, 22, 15,  9, 16,  5, 28, 11,  8, 20, 13,  7, 20, 15, 24, 20,  2, 13,\n",
            "        30, 11, 10,  0, 14, 20, 18,  5, 14,  0,  5, 11, 30,  8, 22,  9, 17, 13,\n",
            "         4, 30, 21,  7,  5, 17, 10,  0,  2, 25, 24,  0, 18, 21, 16, 14,  8, 19,\n",
            "        27, 15,  8, 25, 30,  5, 21, 26, 22, 23, 27, 26, 18,  3, 12,  2, 11, 15,\n",
            "         6,  0, 19,  1,  9,  8, 26, 20, 10, 27, 23, 24, 15,  2, 16, 22, 14, 27,\n",
            "         0,  8, 28,  5,  2, 30, 18, 22, 12, 30,  5, 28, 22,  0, 25,  5,  8, 26,\n",
            "        22,  3, 16, 11, 18, 25,  2,  6, 13, 29, 17, 15, 24, 14, 22, 14, 24, 27,\n",
            "        25, 11, 28, 28, 17, 13, 12, 19, 10,  8, 20,  9,  1,  7, 21,  4, 12, 26,\n",
            "         3,  6,  7, 11,  6, 29,  9, 22, 16,  7,  0,  7, 13,  7, 23, 13, 18, 16,\n",
            "         1,  3, 13,  0, 25,  6, 19, 18, 23,  8, 11, 20, 24, 25, 17, 16,  1, 14,\n",
            "        21, 26, 28,  4,  1,  5,  4, 22, 22, 26, 19,  8,  6,  8,  3,  3, 28, 29,\n",
            "         3, 23, 13,  3,  9, 29,  7, 24, 13, 26, 24,  0, 28,  6,  2, 27, 18, 17,\n",
            "        21,  7, 10,  5, 30, 20, 29,  2, 10,  8, 30, 28, 25, 11, 27, 25,  8, 22,\n",
            "         1,  7, 10,  9, 19, 24, 27, 29, 29, 16, 20,  5, 12, 13, 29, 30, 27, 12,\n",
            "         0, 20, 28,  2,  4,  1,  7,  2, 19,  4, 14, 15, 24, 18,  7, 12,  5, 30,\n",
            "        16, 28, 22, 22, 23,  4, 18,  1, 29, 23, 24, 13, 11, 19,  9, 25, 23, 19,\n",
            "        25,  9,  5,  1,  2, 10,  3, 12, 14,  3,  4, 16, 25, 17,  4,  2, 17,  5,\n",
            "        17,  9, 18, 26,  0,  4, 25, 17,  9,  8, 10, 12,  6, 29, 24, 23,  9, 26,\n",
            "         7, 16,  6, 14, 11, 27, 25, 13, 27, 16, 23, 21, 22,  7, 10,  6, 26, 21,\n",
            "         1, 26, 28, 20, 21,  8, 22,  9,  2, 18, 22,  3, 18, 22, 14, 19,  3, 17,\n",
            "        21,  4, 10,  4, 27, 27, 12, 28, 20, 12, 20, 28, 26, 15, 23, 25, 29, 27,\n",
            "        10,  3,  9, 30, 17,  0,  7, 19])\n",
            "test accuracy: 0.758065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gh2TLKZAFdej"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}